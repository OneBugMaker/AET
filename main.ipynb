{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbde93ae",
   "metadata": {},
   "source": [
    "## 内容：\n",
    "基于transformer分类故障，再利用注意力权重做可解释。\n",
    "\n",
    "\n",
    "## 数据：\n",
    "CWRU： 正常 + 3种故障程度*（外圈+内圈+滚子） = 10种类别\n",
    "\n",
    "B022:  正常 + 2种故障程度*（外圈+内圈+滚子）= 7种类别\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4214b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:51:32.211461Z",
     "start_time": "2023-10-26T13:51:24.916467Z"
    }
   },
   "outputs": [],
   "source": [
    "# 22-9-5\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import scipy.io as scio\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import pywt\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "print(tf.__version__, tf.config.experimental.get_device_details(tf.config.list_physical_devices('GPU')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b41c1",
   "metadata": {},
   "source": [
    "# 全局设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767db6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:01.076747Z",
     "start_time": "2023-10-26T13:52:01.061230Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(plt.rcParams.keys())\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family':   'Times New Roman'   ,   # monospace   \"Times New Roman\"  cursive\n",
    "    \n",
    "    'font.size': 10,\n",
    "    \n",
    "    'figure.dpi': 150,         \n",
    "    # dpi=150,显示得图片大小合适，保存时，设置更高dpi即可\n",
    "    \n",
    "    'axes.titlesize':  10,\n",
    "    'axes.labelsize':  10,\n",
    "    \n",
    "    'legend.fontsize':  9,\n",
    "    'legend.edgecolor': '0.5',\n",
    "    'legend.fancybox':  False,\n",
    "    'legend.markerscale': 1.0,\n",
    "    'legend.frameon': False,\n",
    "    \n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    \n",
    "    'grid.alpha': 0.5,\n",
    "    'grid.linestyle': '--',\n",
    "    \n",
    "    \n",
    "    'savefig.dpi': 600,\n",
    "    \n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "# 22-9-5\n",
    "def get_date():\n",
    "    now_time = list(time.localtime(time.time()))\n",
    "    now_time = '_TIME-({0:4d}-{1:02d}-{2:02d})-({3:02d}-{4:02d}-{5:02d})_'.format(\n",
    "                now_time[0], now_time[1], now_time[2], \n",
    "                now_time[3], now_time[4], now_time[5], )\n",
    "    return now_time\n",
    "now_time = get_date()\n",
    "print(now_time)\n",
    "\n",
    "\n",
    "Health_status = ['NC','OF1','IF1','RF1',   'OF2','IF2','RF2',   'OF3','IF3','RF3']\n",
    "\n",
    "def normalF(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    new_x = (x-mean)/(std + 1e-6)\n",
    "    return new_x\n",
    "\n",
    "def scaleF(x):\n",
    "    mi  = np.min(x)\n",
    "    ma  = np.max(x)\n",
    "    new_x = (x-mi) / (ma-mi + 1e-6)\n",
    "    new_x = new_x*2 - 1\n",
    "    return new_x\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "CWT_SCAL_NUM = 128\n",
    "\n",
    "IMG_WIDTH=256\n",
    "IMG_HEIGHT=64\n",
    "\n",
    "SIGNAL_LENGTH = 2048\n",
    "\n",
    "dta_R_date = 'TEST_'\n",
    "save_path  = './save/temp/File{}/'.format(now_time)\n",
    "\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    save_path = save_path + 'save/'\n",
    "    os.mkdir(save_path)\n",
    "    print(f\"文件夹 {save_path} 创建成功！\")\n",
    "else:\n",
    "    print(f\"文件夹 {save_path} 已存在。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991bff47",
   "metadata": {},
   "source": [
    "# 数据观察\n",
    "## BJUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470eb76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:07.480332Z",
     "start_time": "2023-10-26T13:52:05.244788Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = 'F:\\\\Data\\\\IT\\\\'\n",
    "\n",
    "BJUT_SET_NAME = [\n",
    "                'BJUT_18mN',\n",
    "                'BJUT_180N',\n",
    "                'BJUT_15mN',\n",
    "                'BJUT_150N',]\n",
    "\n",
    "CWRU_SET_NAME = [\n",
    "                'CWRU_0HP',\n",
    "                'CWRU_1HP',\n",
    "                'CWRU_2HP',\n",
    "                'CWRU_3HP',]\n",
    "\n",
    "FOLDER_LIST = BJUT_SET_NAME + CWRU_SET_NAME\n",
    "\n",
    "\n",
    "\n",
    "PATH_DICT = {key:[ROOT_PATH+key+'\\\\'+file for file in os.listdir(ROOT_PATH + key)] for key in FOLDER_LIST}\n",
    "\n",
    "PATH_DICT.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895f93d",
   "metadata": {},
   "source": [
    "# load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2ad06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:13.541848Z",
     "start_time": "2023-10-26T13:52:09.181366Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(folder, file_type, strides=1):\n",
    "    assert file_type in ['csv', 'jpg', 'both']\n",
    "    all_filenemas = np.array(PATH_DICT[folder])   # 包含CSV文件（振动信号）和JPG文件（CWT图）\n",
    "    \n",
    "    '''# 当文件类型为'csv'  或者 'both' 时要用到CSV文件，所以读取CSV文件'''\n",
    "    if file_type in ['csv',  'both']:\n",
    "        filter_indexes = [file[-3:]=='csv' for file in all_filenemas ]  # 找出所有的CSV文件\n",
    "        filenemas  = all_filenemas[filter_indexes][::strides]            # 用strides控制间隔取样的间隔距离\n",
    "        data_csv   = np.array([pd.read_csv(file).values for file in filenemas])\n",
    "        labels_csv = np.array([int(file.split('CLS_')[1][0]) for file in filenemas])\n",
    "\n",
    "\n",
    "    if file_type in ['jpg',  'both']:\n",
    "        filter_indexes = [file[-3:]=='jpg' for file in all_filenemas ]\n",
    "        filenemas  = all_filenemas[filter_indexes][::strides]\n",
    "        data_jpg   = np.array([plt.imread(file)[:,:,0] for file in filenemas])\n",
    "        data_jpg   = np.transpose(data_jpg, axes=[0,2,1])\n",
    "        data_jpg   = data_jpg / 255.\n",
    "        labels_jpg = np.array([int(file.split('CLS_')[1][0]) for file in filenemas])\n",
    "    \n",
    "    \n",
    "    ''' # 根据具体的文件类型，来处理输出是单类文件，还是两种文件的融合（某个维度上拼接）'''\n",
    "    if file_type == 'csv':\n",
    "        print(folder, file_type, data_csv.shape,labels_csv.shape, labels_csv[::500], sep='\\t')\n",
    "        return data_csv, labels_csv\n",
    "    \n",
    "    if file_type == 'jpg':\n",
    "        print(folder, file_type, data_jpg.shape,labels_jpg.shape, labels_jpg[::500], sep='\\t')\n",
    "        return data_jpg, labels_jpg\n",
    "    \n",
    "    if file_type == 'both':\n",
    "        data = [np.concatenate([data_jpg[i], data_csv[i].reshape(-1,64)]) \n",
    "                                for i in range(len(data_jpg))]\n",
    "        data = np.array(data)\n",
    "        labels =  labels_jpg\n",
    "        print(folder, file_type, data.shape, labels.shape,  labels[::500],  sep='\\t')\n",
    "        return data,  labels\n",
    "    \n",
    "\n",
    "# temp_data, temp_labels = load_data( folder='BJUT_18mN', file_type='csv',  strides=10  )\n",
    "# temp_data, temp_labels = load_data( folder='CWRU_0HP',  file_type='jpg',  strides=10  )\n",
    "temp_data, temp_labels = load_data( folder='BJUT_18mN', file_type='both', strides=10  )\n",
    "cwt_data = temp_data[::50,:256]\n",
    "sig_data = temp_data[::50,256:].reshape(-1,2048,1)\n",
    "\n",
    "print(np.mean(cwt_data),  np.std(cwt_data),  np.max(cwt_data))\n",
    "print(np.mean(sig_data),  np.std(sig_data),  np.max(sig_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0665a",
   "metadata": {},
   "source": [
    "## plot_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c0b94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:13.712815Z",
     "start_time": "2023-10-26T13:52:13.574506Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_signal(x, t=None, figsize=(4,0.8),tit=None,xlb='Time(s)',ylb='Amplitude',\n",
    "                axis_off=False, sp=None):\n",
    "    plt.figure(figsize=figsize )\n",
    "    if t is not None:\n",
    "        plt.plot( t, x, linewidth=0.7 )\n",
    "        plt.xlim(t[0],t[-1])\n",
    "    else:\n",
    "        plt.plot( x, linewidth=0.7 )\n",
    "        plt.xlim( 0,  len(x)  )\n",
    "    \n",
    "    if xlb is not None: plt.xlabel(xlb )\n",
    "    if ylb is not None: plt.ylabel(ylb )\n",
    "    if tit is not None: plt.title( tit )\n",
    "    if axis_off:          plt.axis('off')\n",
    "    if sp  is not None: \n",
    "        plt.savefig(sp + tit + '.png', bbox_inches ='tight')\n",
    "        plt.savefig(sp + tit + '.svg', bbox_inches ='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def show_matrix(x, figsize=(3,1),tit=None,xlb=None,ylb=None,axis_off=False,need_colorbar=False,sp=None):\n",
    "    plt.figure(figsize=figsize )\n",
    "    plt.contourf( x, levels=16, cmap=None)\n",
    "    \n",
    "    if xlb is not None: plt.xlabel(xlb )\n",
    "    if ylb is not None: plt.ylabel(ylb )\n",
    "    if tit is not None: plt.title( tit )\n",
    "    if need_colorbar:     plt.colorbar(shrink =0.99)\n",
    "    if axis_off:          plt.axis('off')\n",
    "    if sp  is not None: \n",
    "        plt.savefig(sp + tit + '.png', bbox_inches ='tight')\n",
    "        plt.savefig(sp + tit + '.svg', bbox_inches ='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def show_attention(x, figsize=(3,1),tit=None,xlb='Position',ylb='Attention',axis_off=False, need_colorbar=False,sp=None):\n",
    "    plt.figure(figsize=figsize )\n",
    "    plt.contourf( x, levels=4, cmap='inferno')\n",
    "    \n",
    "    if xlb is not None: plt.xlabel(xlb )\n",
    "    if ylb is not None: plt.ylabel(ylb )\n",
    "    if tit is not None: plt.title( tit )\n",
    "    if need_colorbar:     plt.colorbar(shrink =0.99)\n",
    "    if axis_off:          plt.axis('off')\n",
    "    if sp  is not None: \n",
    "        plt.savefig(sp + tit + '.png', bbox_inches ='tight')\n",
    "        plt.savefig(sp + tit + '.svg', bbox_inches ='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# temp_cwt = np.arange(256*64).reshape(64,256)\n",
    "# show_matrix(temp_cwt, tit='temp_CWT-without-tf', axis_off=False,  need_colorbar=True,sp='./save/')\n",
    "\n",
    "# show_attention(temp_cwt, tit='temp_CWT-without-tf', axis_off=False,  need_colorbar=True,sp='./save/')\n",
    "\n",
    "def plot_qk_curves(q,k, figsize=(3,1),tit=None,xlb='Position',ylb='Attention',axis_off=False, sp=None):\n",
    "    plt.figure(figsize=figsize )\n",
    "    plt.plot( q,label='Query' )\n",
    "    plt.plot( k,label='Key' )\n",
    "    plt.legend()\n",
    "    plt.xlim(0,len(q))\n",
    "    \n",
    "    if xlb is not None: plt.xlabel(xlb )\n",
    "    if ylb is not None: plt.ylabel(ylb )\n",
    "    if tit is not None: plt.title( tit )\n",
    "    if axis_off:          plt.axis('off')\n",
    "    if sp  is not None: \n",
    "        plt.savefig(sp + tit + '.png', bbox_inches ='tight')\n",
    "        plt.savefig(sp + tit + '.svg', bbox_inches ='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# x = np.arange(100)\n",
    "# y = np.arange(100)*2\n",
    "# plot_qk_curves(x,y, tit='qk' , axis_off=False,  sp='./save/')\n",
    "\n",
    "plot_signal(sig_data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152dc7c",
   "metadata": {},
   "source": [
    "# get_tra_tes_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a76870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:15.154648Z",
     "start_time": "2023-10-26T13:52:13.762407Z"
    }
   },
   "outputs": [],
   "source": [
    "traXs,tesXs,  traYs,tesYs = train_test_split(temp_data, temp_labels,   test_size=0.20, shuffle=True, random_state=19960103 )\n",
    "\n",
    "# 22-9-5\n",
    "def get_tra_tes_ds(traXs,tesXs,  traYs,tesYs, BS=BATCH_SIZE,  is_train=False):\n",
    "    [traXs,tesXs,  traYs,tesYs] = [np.array( item ) for item in [traXs,tesXs,  traYs,tesYs]] # list格式转成np.array好索引\n",
    "    print('traXs.shape,  traYs.shape, traYs[:10]\\t',traXs.shape,  traYs.shape, traYs[:10],sep='\\t')\n",
    "    print('tesXs.shape,  tesYs.shape, tesYs[:10]\\t',tesXs.shape,  tesYs.shape, tesYs[:10],sep='\\t')\n",
    "    tra_ds = tf.data.Dataset.from_tensor_slices((traXs,traYs)).repeat().shuffle(len(traXs)).batch(BS, drop_remainder=True).prefetch(2)\n",
    "    tes_ds = tf.data.Dataset.from_tensor_slices((tesXs,tesYs)).repeat().shuffle(len(traXs)).batch(BS).prefetch(2)\n",
    "    return tra_ds, tes_ds\n",
    "\n",
    "tra_ds, tes_ds = get_tra_tes_ds(traXs,tesXs,  traYs,tesYs)\n",
    "\n",
    "tra_bX, tra_bY = next(iter(tra_ds))\n",
    "print('tra_bX.shape, tra_bY.shape, tra_bY.numpy()[:10]',tra_bX.shape, tra_bY.shape, tra_bY.numpy()[:10] ,sep='\\t')\n",
    "\n",
    "tes_bX, tes_bY = next(iter(tes_ds))\n",
    "print('tes_bX.shape, tes_bY.shape, tes_bY.numpy()[:10]',tes_bX.shape, tes_bY.shape, tes_bY.numpy()[:10],sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2cc84",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893cbc00",
   "metadata": {},
   "source": [
    "## get_batch_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c496b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:18.807342Z",
     "start_time": "2023-10-26T13:52:15.203509Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_batch_from_path(set_flag='BJUT_18mN', file_type='csv', strides=5, # STRIDES为5时，每类有100样本\n",
    "                        train_rate=0.2, seed = 19960103):\n",
    "    '''本项目里面，数据加载的最高级封装，\n",
    "    依据数据集的名字和工况加载对应数据，\n",
    "    CWRU：加载对应工况下，正常 + 3程度*3故障类别/程度 的 10 类数据\n",
    "    B02022：加载对应工况下，正常 + 2程度*3故障类别/程度 的 7 类数据\n",
    "    加载好的数据，需要返回划分好的traXs,traYs, tesXs,tesYs\n",
    "    以及，tra_ds, tes_ds\n",
    "    '''\n",
    "    assert set_flag in FOLDER_LIST\n",
    "    \n",
    "    \n",
    "    # 加载数据\n",
    "    all_Xs, all_Ys = load_data( folder=set_flag, file_type=file_type, strides=strides  )\n",
    "    if 'BJUT' in set_flag:\n",
    "        CLS_NUM = 7\n",
    "    if 'CWRU' in set_flag:\n",
    "        CLS_NUM =  10\n",
    "    print('CLS_NUM = ', CLS_NUM)\n",
    "        \n",
    "    traXs,tesXs,  traYs,tesYs = train_test_split(all_Xs, all_Ys,   test_size=1-train_rate, \n",
    "                                                 shuffle=True, random_state= seed,\n",
    "                                                stratify=all_Ys\n",
    "                                                )\n",
    "    print('Train number = {0:5d}, Test number = {1:5d}'.format(len(traXs),  len(tesXs)))\n",
    "    print(Counter(traYs),Counter(tesYs), sum(tesYs)/sum(traYs))\n",
    "    \n",
    "    tra_ds, tes_ds = get_tra_tes_ds(traXs,tesXs,  traYs,tesYs)\n",
    "\n",
    "    tra_bX, tra_bY = next(iter(tra_ds))\n",
    "    print('tra_bX.shape, tra_bY.shape, tra_bY.numpy()[:10]',tra_bX.shape, tra_bY.shape, tra_bY.numpy()[:10] ,sep='\\t')\n",
    "\n",
    "    tes_bX, tes_bY = next(iter(tes_ds))\n",
    "    print('tes_bX.shape, tes_bY.shape, tes_bY.numpy()[:10]',tes_bX.shape, tes_bY.shape, tes_bY.numpy()[:10],sep='\\t')\n",
    "    print('Dataset OK!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\\n')\n",
    "    \n",
    "    return (traXs,tesXs,  traYs,tesYs), (tra_ds, tes_ds), CLS_NUM\n",
    "\n",
    "\n",
    "(traXs,tesXs,  traYs,tesYs), (tra_ds, tes_ds), CLS_NUM = get_batch_from_path( set_flag = 'BJUT_18mN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e57050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:23.745764Z",
     "start_time": "2023-10-26T13:52:22.675216Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,(bx,by) in enumerate(tra_ds.take(8)):\n",
    "    \n",
    "    noise = np.random.normal(loc=0.0, scale=0.50, size=bx.shape)\n",
    "    \n",
    "    x1 = bx[0].numpy()\n",
    "    \n",
    "    bx2 = bx + noise\n",
    "    x2 = bx2[0].numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8,1),dpi=100)\n",
    "    plt.plot(x2, label='x2_noise', linewidth=0.5)\n",
    "    plt.plot(x1, label='x1_raw', linewidth=0.5)\n",
    "    \n",
    "#     plt.xlim(0,1000)\n",
    "    plt.legend()\n",
    "    plt.ylabel(by[0].numpy())\n",
    "    plt.show()\n",
    "    print(i,bx.shape, noise.shape, by.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd17580",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "##   MLP + BiLSTM + CNN + ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1072219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:27.547519Z",
     "start_time": "2023-10-26T13:52:26.841519Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions.models_building import get_CNN, get_ResNet, get_HCM, get_MLP, get_BiLSTM\n",
    "\n",
    "CWT_SHAPE = (256,64)\n",
    "SIG_SHAPE = (2048,1)\n",
    "MLP = get_MLP(input_shape = SIG_SHAPE )\n",
    "# print( MLP.summary() )\n",
    "print('MLP', MLP.input_shape, MLP.output_shape , MLP.count_params())\n",
    "\n",
    "LSTM = get_BiLSTM(input_shape = SIG_SHAPE)\n",
    "# print( LSTM.summary() )\n",
    "print('LSTM', LSTM.input_shape, LSTM.output_shape , LSTM.count_params())\n",
    "\n",
    "CNN = get_CNN(input_shape = SIG_SHAPE)\n",
    "# print( CNN.summary() )\n",
    "print('CNN', CNN.input_shape, CNN.output_shape, CNN.count_params())\n",
    "\n",
    "\n",
    "ResNet = get_ResNet(input_shape = SIG_SHAPE)\n",
    "# print( ResNet.summary()  )\n",
    "print('Res', ResNet.input_shape, ResNet.output_shape , ResNet.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2f771",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05967f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:51.460271Z",
     "start_time": "2023-10-26T13:52:46.595622Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from functions.AET_model_building import get_ST,get_NET,get_AET\n",
    "from functions.AET_models import get_ST,get_NET,get_AET\n",
    "\n",
    "x = np.ones(shape=(7,2048,1))\n",
    "st = get_ST(num_layers=3, need_show=True)\n",
    "\n",
    "st(x).shape, st.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9194ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:51.739573Z",
     "start_time": "2023-10-26T13:52:51.523858Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = get_NET(num_layers=3, need_show=True)\n",
    "net(x).shape, net.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f706bbff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:53.400122Z",
     "start_time": "2023-10-26T13:52:52.543125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aet = get_AET(num_layers=3, need_show=True)\n",
    "aet(x)[0].shape,aet(x)[1].shape, aet.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239873d",
   "metadata": {},
   "source": [
    "## get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ac3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:55.287330Z",
     "start_time": "2023-10-26T13:52:55.035220Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 22-10-12: 都是 1D 信号，ST的一步下采样为线性32，NET多步下采样为非线性32，AET多步下采样为非线性32,并自监督学习\n",
    "def get_model(  FEM_name,   CLS_NUM, num_layers=6  ):\n",
    "    print('''GGGGGGGGGGGGGGGet Modellllllllllll \\n FEM_name={},   CLS_NUM={},  need_BA={}\n",
    "    '''.format(  FEM_name,   CLS_NUM,  None)  )\n",
    "    assert FEM_name in ['MLP', 'BiLSTM',   'CNN','ResNet',   'ST','NET','AET'  ]\n",
    "    shape1 = (SIGNAL_LENGTH,1)\n",
    "    shape2 = (IMG_WIDTH, IMG_HEIGHT)\n",
    "    \n",
    "    '''特征提取器模型：FEM ''' \n",
    "    if FEM_name == 'MLP':\n",
    "        FEM = get_MLP(input_shape = shape1 )\n",
    "        feature_dim = FEM.output_shape[-1]\n",
    "    if FEM_name == 'BiLSTM':\n",
    "        FEM = get_BiLSTM(input_shape = shape1 )\n",
    "        feature_dim = FEM.output_shape[-1]\n",
    "    if FEM_name == 'CNN':\n",
    "        FEM = get_CNN(input_shape = shape1 )\n",
    "        feature_dim = FEM.output_shape[-1]\n",
    "    if FEM_name == 'ResNet':\n",
    "        FEM = get_ResNet(input_shape =shape1 )\n",
    "        feature_dim = FEM.output_shape[-1]\n",
    "    \n",
    "        \n",
    "    if FEM_name == 'ST':\n",
    "        FEM =   get_ST(num_layers=num_layers, need_show=True)\n",
    "        feature_dim = FEM.out_shape[-1]\n",
    "    if FEM_name == 'NET':\n",
    "        FEM =   get_NET(num_layers=num_layers, need_show=True)\n",
    "        feature_dim = FEM.out_shape[-1]\n",
    "    if FEM_name == 'AET':\n",
    "        FEM =   get_AET(num_layers=num_layers, need_show=True)\n",
    "        feature_dim = FEM.out_shape[-1]\n",
    "    print('basic FEM  is ',FEM.name, '\\t feature_dim = ',feature_dim  )\n",
    "    \n",
    "\n",
    "    '''健康分类器模型：Health classifier model'''\n",
    "    HCM  =  get_HCM(dim_in = feature_dim, dim_out = CLS_NUM)\n",
    "\n",
    "    return FEM, HCM\n",
    "\n",
    "\n",
    "temp_models = get_model(  FEM_name = 'AET',  CLS_NUM  = CLS_NUM,  )\n",
    "temp_FEM, temp_HCM = temp_models\n",
    "temp_inputs = np.ones((7,2048,1))\n",
    "\n",
    "temp_FEM_out = temp_FEM(temp_inputs, training=True)\n",
    "print( 'temp_FEM_out[0].shape ',    temp_FEM_out[0].shape  )\n",
    "print( 'temp_FEM_out[1].shape ',    temp_FEM_out[1].shape  )\n",
    "\n",
    "print( 'temp_HCM.input_shape ',  temp_HCM.input_shape  )\n",
    "temp_FEM.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f6d9b",
   "metadata": {},
   "source": [
    "# Optimizer, Loss and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9074d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:56.422535Z",
     "start_time": "2023-10-26T13:52:56.380341Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_macro_F1(y_true, y_pred, cls_num):\n",
    "    '''多分类的Macro F1 的计算，\n",
    "    在这里，y_true是真实标签，y_pred是预测标签。\n",
    "    注意：y_true, y_true均是普通数字标签而不是onehot格式\n",
    "    （如若有3类的话，为【0，1，2】，不是【【1，0，0】，【0，1，0】，【0，0，1】】）\n",
    "    '''\n",
    "    y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.int32)\n",
    "    \n",
    "    # calculate per-class precision, recall, and F1\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    # loop over each class\n",
    "    for class_idx in range(cls_num):\n",
    "        # convert y_true and y_pred to binary arrays for the current class\n",
    "        class_true = tf.equal(y_true, class_idx)\n",
    "        class_pred = tf.equal(y_pred, class_idx)\n",
    "\n",
    "        # count true positive, false positive, and false negative for the class\n",
    "        true_positives  = tf.reduce_sum(tf.cast(tf.math.logical_and(class_true, class_pred), tf.int32))\n",
    "        false_positives = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(class_true), class_pred), tf.int32))\n",
    "        false_negatives = tf.reduce_sum(tf.cast(tf.math.logical_and(class_true, tf.math.logical_not(class_pred)), tf.int32))\n",
    "\n",
    "        # calculate precision, recall, and F1 for the class\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "#     print([i.numpy() for i in f1s])\n",
    "    # calculate macro-average F1 score\n",
    "    macro_precisions = tf.reduce_mean(precisions)*100\n",
    "    macro_recalls = tf.reduce_mean(recalls)*100\n",
    "    macro_f1s = tf.reduce_mean(f1s)*100\n",
    "    return macro_f1s, macro_precisions, macro_recalls\n",
    "\n",
    "\n",
    "y_true  = [0, 1, 2, 0, 1, 2]\n",
    "y_pred  = [0, 1, 2, 0, 0, 1]\n",
    "\n",
    "# count number of classes\n",
    "cls_num = 3\n",
    "\n",
    "macro_f1s, macro_precisions, macro_recalls = get_macro_F1(y_true, y_pred, cls_num)\n",
    "macro_f1s, macro_precisions, macro_recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b6735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:52:57.137385Z",
     "start_time": "2023-10-26T13:52:57.100177Z"
    }
   },
   "outputs": [],
   "source": [
    "'''这里定义的LOSS和METRIC都是全局变量，\n",
    "尤其是 METRIC ，每次需要从零开始时需要 reset_metrics\n",
    "'''\n",
    "optimizer  = tf.keras.optimizers.Adam( 0.0005 )\n",
    "\n",
    "metric_name_list = ['CE', 'MSE', 'Acc(%)', 'F1(%)', 'P(%)', 'R(%)']\n",
    "\n",
    "# 交叉熵损失函数用来是衡量 预测误差\n",
    "CE_loss  = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "MSE_loss = tf.keras.losses.MeanSquaredError( )\n",
    "\n",
    "# train_step 中 loss 在求梯度时已经算过了，所以只需要把求得的loss记录即可, 但 acc 需要单独计算\n",
    "train_CE_metric  = tf.keras.metrics.Mean( name='train_CE_metric')  \n",
    "train_MSE_metric = tf.keras.metrics.Mean( name='train_MSE_metric')  \n",
    "train_ACC_metric = tf.keras.metrics.SparseCategoricalAccuracy(     name='train_ACC_metric') \n",
    "train_F1_metric  = tf.keras.metrics.Mean( name='train_F1_metric')  \n",
    "train_P_metric   = tf.keras.metrics.Mean( name='train_P_metric')  \n",
    "train_R_metric   = tf.keras.metrics.Mean( name='train_R_metric')  \n",
    "\n",
    "\n",
    "# test_step 中， 需要 单独计算 loss 和 acc\n",
    "test_CE_metric   = tf.keras.metrics.Mean( name='test_CE_metric')\n",
    "test_MSE_metric  = tf.keras.metrics.Mean( name='test_MSE_metric')  \n",
    "test_ACC_metric  = tf.keras.metrics.SparseCategoricalAccuracy(     name='test_ACC_metric')\n",
    "test_F1_metric   = tf.keras.metrics.Mean( name='test_F1_metric')  \n",
    "test_P_metric    = tf.keras.metrics.Mean( name='test_P_metric')  \n",
    "test_R_metric    = tf.keras.metrics.Mean( name='test_R_metric')\n",
    "\n",
    "#  train metrics\n",
    "def reset_train_metrics():\n",
    "    train_CE_metric.reset_state()    # 训练 健康分类器的交叉熵\n",
    "    train_MSE_metric.reset_state()   # 训练 自编码器的MSE\n",
    "    train_ACC_metric.reset_state()   # 训练 健康分类器的准确率\n",
    "    train_F1_metric.reset_state()    # 训练 健康分类器的 F1\n",
    "    train_P_metric.reset_state()     # 训练 健康分类器的 P\n",
    "    train_R_metric.reset_state()     # 训练 健康分类器的 R\n",
    "    \n",
    "def get_train_metrics_result():\n",
    "    CE   = train_CE_metric.result().numpy()\n",
    "    MSE  = train_MSE_metric.result().numpy()\n",
    "    ACC  = train_ACC_metric.result().numpy()*100\n",
    "    F1   = train_F1_metric.result().numpy()\n",
    "    Pr   = train_P_metric.result().numpy()\n",
    "    Re   = train_R_metric.result().numpy()\n",
    "    return [CE,MSE,ACC,F1,Pr,Re]\n",
    "\n",
    "#  test metrics\n",
    "def reset_test_metrics():\n",
    "    test_CE_metric.reset_state()     # 测试 健康分类器的交叉熵\n",
    "    test_MSE_metric.reset_state()    # 测试 自编码器的MSE\n",
    "    test_ACC_metric.reset_state()    # 测试 健康分类器的准确率\n",
    "    test_F1_metric.reset_state()     # 测试 健康分类器的 F1\n",
    "    test_P_metric.reset_state()      # 测试 健康分类器的 P\n",
    "    test_R_metric.reset_state()      # 测试 健康分类器的 R\n",
    "    \n",
    "def get_test_metrics_result():\n",
    "    CE  = test_CE_metric.result().numpy()\n",
    "    MSE = test_MSE_metric.result().numpy()\n",
    "    ACC = test_ACC_metric.result().numpy()*100\n",
    "    F1  = test_F1_metric.result().numpy()\n",
    "    Pr   = test_P_metric.result().numpy()\n",
    "    Re   = test_R_metric.result().numpy()\n",
    "    return [CE,MSE,ACC,F1,Pr,Re]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a658b8d",
   "metadata": {},
   "source": [
    "# train_step and test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c442b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:00.066755Z",
     "start_time": "2023-10-26T13:52:58.707989Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''2023-2-6\n",
    "一个  tf.GradientTape()  监督多个子模型的可训练变量，\n",
    "模型里面的每个张量，都是以列表的格式存储，\n",
    "因此两个模型的张量梯度列表可以串联起来，\n",
    "然后，对合起来的张量列表里的每个张量求一次梯度，\n",
    "得到每个张量的梯度列表，\n",
    "最后 根据梯度列表，给每个张量更新参数\n",
    "'''\n",
    "\n",
    "def train_step(X, Y, models, w_ce=1.0, w_mse=0.0):\n",
    "    #print('X.shape, Y.shape',    X.shape, Y.shape, )\n",
    "    FEM, HCM = models\n",
    "    noise = np.random.normal(loc=0.0, scale=0.5, size=X.shape)\n",
    "    X = X + noise\n",
    "    with tf.GradientTape() as tape:\n",
    "        if FEM.name=='AET': \n",
    "            XRC,F = FEM( X, training = True)\n",
    "            P   = HCM( F, training = True)\n",
    "            # print('X.shape, Y.shape, F.shape, P.shape',X.shape, Y.shape, F.shape, P.shape  )\n",
    "            ce  = CE_loss(y_true=Y, y_pred=P )\n",
    "            mse = MSE_loss(y_true=X, y_pred=XRC)\n",
    "            loss = w_ce*ce + w_mse*mse\n",
    "            \n",
    "        else:\n",
    "            F   = FEM( X, training = True)                    # 提取的 特征          (bs, dim)\n",
    "            P   = HCM( F, training = True)                    # 预测 健康状态        (bs, 10)  \n",
    "            ce  = CE_loss(y_true=Y, y_pred=P )\n",
    "            mse = 0.0\n",
    "            loss = ce\n",
    "    \n",
    "    trainable_variables = FEM.trainable_variables + HCM.trainable_variables\n",
    "    # FEM.trainable_variables 以列表的形式存储模型里的每个可训练参数\n",
    "    gradients  = tape.gradient( loss,          trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,   trainable_variables))\n",
    "    \n",
    "    P_argmax = tf.argmax( P, axis=-1)\n",
    "    cls_num  = tf.shape( P )[-1]\n",
    "    F1,Pr,Re = get_macro_F1(y_true=Y, y_pred=P_argmax, cls_num=cls_num)\n",
    "    \n",
    "    train_CE_metric.update_state( ce )\n",
    "    train_MSE_metric.update_state( mse )\n",
    "    train_ACC_metric.update_state( y_true=Y, y_pred=P  )\n",
    "    train_F1_metric.update_state( F1 )\n",
    "    train_P_metric.update_state( Pr )\n",
    "    train_R_metric.update_state( Re )\n",
    "    return (FEM, HCM)\n",
    "\n",
    "\n",
    "def test_step(X, Y, models):\n",
    "    FEM, HCM = models\n",
    "    \n",
    "    if FEM.name=='AET': \n",
    "        XRC,F = FEM( X, training = False)\n",
    "        P   = HCM( F, training = False)\n",
    "        # print('X.shape, Y.shape, F.shape, P.shape',X.shape, Y.shape, F.shape, P.shape  )\n",
    "        ce  = CE_loss(y_true=Y, y_pred=P )\n",
    "        mse = MSE_loss(y_true=X, y_pred=XRC)\n",
    "\n",
    "    else:\n",
    "        F   = FEM( X, training = False)                    # 提取的 特征          (bs, dim)\n",
    "        P   = HCM( F, training = False)                    # 预测 健康状态        (bs, 10)  \n",
    "        ce  = CE_loss(y_true=Y, y_pred=P )\n",
    "        mse = 0.0\n",
    "    \n",
    "    P_argmax = tf.argmax( P, axis=-1)\n",
    "    cls_num  = tf.shape( P )[-1]\n",
    "    F1,Pr,Re = get_macro_F1(y_true=Y, y_pred=P_argmax, cls_num=cls_num)\n",
    "    \n",
    "    test_CE_metric.update_state(  ce )\n",
    "    test_MSE_metric.update_state( mse )\n",
    "    test_ACC_metric.update_state( y_true=Y, y_pred=P )\n",
    "    test_F1_metric.update_state( F1 )\n",
    "    test_P_metric.update_state( Pr )\n",
    "    test_R_metric.update_state( Re )\n",
    "    return\n",
    "\n",
    "(traXs,tesXs,  traYs,tesYs), (tra_ds, tes_ds), CLS_NUM = get_batch_from_path( set_flag = 'BJUT_18mN',  file_type='csv', strides=10)\n",
    "temp_models = get_model(      FEM_name='AET',   CLS_NUM=CLS_NUM,    )\n",
    "\n",
    "bX, bY = next(iter(tra_ds))\n",
    "\n",
    "\n",
    "train_step(  X = bX, Y = bY,  models=temp_models  )\n",
    "test_step(   X = bX, Y = bY,  models=temp_models  )\n",
    "\n",
    "def print_log(metric_results, need_show_metric_name=False, metric_names = metric_name_list, ):\n",
    "    if need_show_metric_name:\n",
    "        [print('{}'.format(i),      end='\\t') for i in metric_names]\n",
    "        print()\n",
    "    [print('{:.2f}'.format(i),  end='\\t') for i in metric_results]\n",
    "    print()\n",
    "    return\n",
    "\n",
    "print_log(get_train_metrics_result(), True)\n",
    "print_log(get_test_metrics_result(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51137aba",
   "metadata": {},
   "source": [
    "# train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a068f35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:01.023562Z",
     "start_time": "2023-10-26T13:53:00.683221Z"
    }
   },
   "outputs": [],
   "source": [
    "# epoch 等设置为全局变量，就不用每次都传参了\n",
    "epochs     = np.arange(100)\n",
    "weight_ce  = 1-0.5*np.sqrt(epochs/100)\n",
    "weight_mse = 0.5 + 0.5*np.exp(-epochs/32)\n",
    "\n",
    "plt.figure(figsize=(2,1),dpi=100)\n",
    "plt.plot(epochs, weight_ce, '-',  label='$W_{CE}$')\n",
    "plt.plot(epochs, weight_mse, '--', label='$W_{MSE}$')\n",
    "plt.legend(ncol=2, loc='lower center',    bbox_to_anchor=(0.50, 1.0),)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Weights')\n",
    "plt.savefig('weights.png',dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca607f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:01.565457Z",
     "start_time": "2023-10-26T13:53:01.556152Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_ce.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f2171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:10.030804Z",
     "start_time": "2023-10-26T13:53:02.077274Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(tra_ds, tes_ds, models, epoch_num=100,\n",
    "                steps_per_epoch=10, val_steps_per_epoch=1, info=None ): \n",
    "    '''从随机初始化的模型开始训练，直到模型收敛'''\n",
    "    train_model_start_time = time.time()\n",
    "    print('train_model 开始了','!'*100)\n",
    "    tra_ds_iter, tes_ds_iter = iter( tra_ds ), iter( tes_ds )\n",
    "    \n",
    "    assert epoch_num <= len(weight_ce)\n",
    "    \n",
    "    tra_tes_metric_logs = []\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        reset_train_metrics() # 重置多个metric的观测器\n",
    "        reset_test_metrics()  # 重置多个metric的观测器\n",
    "\n",
    "        '''---------Training------------------'''\n",
    "        for step in range(steps_per_epoch):\n",
    "            bX,bY = next(tra_ds_iter)\n",
    "            models = train_step(X=bX, Y=bY, \n",
    "                                models=models,\n",
    "                                w_ce=weight_ce[epoch],\n",
    "                                w_mse=weight_mse[epoch])\n",
    "\n",
    "        '''----------Testing-------------------'''\n",
    "        for step in range(val_steps_per_epoch):\n",
    "            bX,bY = next(tes_ds_iter)\n",
    "            test_step(X=bX, Y=bY, models=models)  # 对验证集的测试\n",
    "            \n",
    "            \n",
    "        '''----------Metrics-------------------'''    \n",
    "        tra_metrics = get_train_metrics_result()\n",
    "        tes_metrics = get_test_metrics_result()\n",
    "        tra_tes_metric_logs.append( tra_metrics + tes_metrics )\n",
    "        \n",
    "        '''----------Print-------------------'''    \n",
    "        if epoch%10==0 or epoch+1==epoch_num:\n",
    "#         if epoch%q==0 :\n",
    "            cinfo = '\\t traN={}, \\ttesN={}, \\t w_ce={:.2f}, \\t w_mse={:.2f}'.format(\n",
    "                steps_per_epoch,val_steps_per_epoch,weight_ce[epoch], weight_mse[epoch])\n",
    "            if info is not None: print(info, cinfo)\n",
    "            print_log(tra_metrics, True)\n",
    "            print_log(tes_metrics, False)\n",
    "            print('Epoch:{:4d}, \\t\\t Epoch time: {:>8.2f}\\n'.format(epoch,     time.time()-epoch_start_time ))\n",
    "       \n",
    "    tra_tes_metric_logs = np.array( tra_tes_metric_logs )  # [max_epoch,  4]\n",
    "    print('Train_model time: {:>8.2f}'.format(       time.time()-train_model_start_time )            )\n",
    "    print('train_model 结束了','!'*100)\n",
    "    return models, tra_tes_metric_logs\n",
    "\n",
    "(traXs,tesXs,  traYs,tesYs), (tra_ds, tes_ds), CLS_NUM = get_batch_from_path(   set_flag = 'BJUT_18mN',  \n",
    "                                                                                file_type='csv', \n",
    "                                                                                strides=5,\n",
    "                                                                                train_rate=0.5,\n",
    "                                                                                seed=19960103,)\n",
    "temp_models = get_model(      FEM_name='AET',   CLS_NUM=CLS_NUM,    )\n",
    "\n",
    "models, tra_tes_metric_logs = train_model(tra_ds, tes_ds, temp_models,  epoch_num=20,\n",
    "                steps_per_epoch=1, val_steps_per_epoch=1, info=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd6f44",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "## Weights、LOG、RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d53bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:15.442103Z",
     "start_time": "2023-10-26T13:53:15.283188Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_weights_and_log():\n",
    "    FEM, HCM = models\n",
    "    \n",
    "    '''存储模型权重'''\n",
    "    item = save_path + get_date() + dta_R_date\n",
    "    for model in models:\n",
    "        model_save_path = item + model.name +'.h5'\n",
    "        model.save_weights(model_save_path)\n",
    "        model.load_weights(model_save_path)\n",
    "    \n",
    "    '''训练日志'''\n",
    "    metric_names = ['CE', 'MSE','Accuracy(%)', 'F1(%)', 'Precision(%)', 'Recall(%)']\n",
    "    columns = ['tra ' + i for i in metric_names] + ['val ' + i for i in metric_names]\n",
    "    \n",
    "    log_df = pd.DataFrame(tra_tes_metric_logs, columns = columns)\n",
    "    log_path = save_path + get_date() + dta_R_date + 'LOG.csv'\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    \n",
    "    print('-'*100, '\\n训练的最后10Epoch的平均Metrics.')\n",
    "    print(log_df.iloc[-10:,].describe().loc[['mean','50%']])\n",
    "    print('训练的最后10Epoch的平均Metrics.\\n','-'*100)\n",
    "    return log_df,log_path\n",
    "    \n",
    "try:\n",
    "    log_df,log_path = save_weights_and_log()\n",
    "except:\n",
    "    print('error')\n",
    "    \n",
    "    \n",
    "def save_test_result(item=''):\n",
    "    '''最终的测试结果'''\n",
    "    tra_CE = tf.keras.metrics.SparseCategoricalCrossentropy(from_logits=True)(  y_true=traYs, y_pred=traPs )\n",
    "    tra_AC = tf.keras.metrics.SparseCategoricalAccuracy()(                       y_true=traYs, y_pred=traPs )*100\n",
    "    traF1,traP,traR = get_macro_F1(y_true=traYs, y_pred=traPargM, cls_num=CLS_NUM )\n",
    "\n",
    "    tes_CE = tf.keras.metrics.SparseCategoricalCrossentropy(from_logits=True)(  y_true=tesYs, y_pred=tesPs )\n",
    "    tes_AC = tf.keras.metrics.SparseCategoricalAccuracy()(                       y_true=tesYs, y_pred=tesPs )*100\n",
    "    tesF1,tesP,tesR = get_macro_F1(y_true=tesYs, y_pred=tesPargM, cls_num=CLS_NUM )\n",
    "\n",
    "    final_result = [ [item.numpy() for item in [  tra_CE,  tra_AC,    traF1,    traP,    traR,\n",
    "                                                   tes_CE,  tes_AC,    tesF1,    tesP,    tesR,   ]]]\n",
    "    result_names =                             [  'tra CE', 'tra AC',  'tra F1', 'tra P', 'tra R',\n",
    "                                                  'tes CE', 'tes AC',  'tes F1', 'tes P', 'tes R',  ]\n",
    "    \n",
    "    final_result_df = pd.DataFrame(  data = final_result,  columns = result_names  )\n",
    "   \n",
    "    path =  save_path + get_date() + dta_R_date + item +'Final_Result.csv'\n",
    "    final_result_df.to_csv(path,  index=False)\n",
    "    \n",
    "    print('='*100, '\\n在训练集和测试集上的最终测试结果.\\n')\n",
    "    print(final_result_df)\n",
    "    print('在训练集和测试集上的最终测试结果.\\n','='*100)\n",
    "    \n",
    "    return final_result_df\n",
    "\n",
    "try:\n",
    "    final_result_df = save_test_result()\n",
    "except:\n",
    "    print('error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e8775",
   "metadata": {},
   "source": [
    "## Metrics curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879d656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:17.807795Z",
     "start_time": "2023-10-26T13:53:16.579626Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_curve(df, metrics,need_tra=True, need_save=True):\n",
    "    '''画出n个metrics的曲线图，metrics必须是list\n",
    "    metric_names = ['SHCE','SHAC',  'THCE','THAC',  'SSC','SDC',  'DCE','DAC','DCS']\n",
    "    '''\n",
    "    assert isinstance(metrics,list)\n",
    "    assert len(metrics) in [1,2]\n",
    "    if 'CE' in str(metrics): assert len(metrics)==1\n",
    "    \n",
    "    plt.subplots(figsize=[2,1.5],)\n",
    "    colors = ['#FF9671','b','#008E9B','r',]\n",
    "    for mi,metric in enumerate(metrics):\n",
    "        if need_tra:\n",
    "            plt.plot(df['tra ' + metric],label='Train' ,  linewidth=1, color=colors[mi*2+0], alpha=0.7)  # 训练曲线\n",
    "        plt.plot(    df['val ' + metric],label='Test' ,   linewidth=1, color=colors[mi*2+1], alpha=1.0)  # 验证曲线\n",
    "    \n",
    "    if need_tra:\n",
    "        plt.legend(ncol=2,\n",
    "               loc='lower center',        \n",
    "               bbox_to_anchor=(0.50, 1.02),              # legend的坐标原点与坐标原点的相对坐标\n",
    "               )\n",
    "    \n",
    "    plt.grid(True, linestyle='--', which='major', color='grey', alpha=.2)\n",
    "    plt.grid(True, linestyle='-.', which='major', color='grey', alpha=.5)\n",
    "\n",
    "    \n",
    "    if 'E' in str(metrics):\n",
    "#         plt.yticks(ticks=np.arange(0, 2,    4)  )\n",
    "        plt.xlabel(xlabel='Epochs' )\n",
    "        plt.ylabel(ylabel=metrics[0] )\n",
    "    else:\n",
    "        plt.yticks(  np.arange(0, 105, 25))\n",
    "        plt.xlabel(xlabel='Epochs' )\n",
    "        plt.ylabel(ylabel=metrics[0] )\n",
    "        \n",
    "        \n",
    "    if need_save:\n",
    "        path = save_path + get_date() + dta_R_date + str(metrics)\n",
    "        plt.savefig(path + '.png',  bbox_inches ='tight')\n",
    "#         plt.savefig(path + '.svg',  bbox_inches ='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "try:\n",
    "    plot_metrics_curve(df= log_df, metrics = ['CE'])\n",
    "    plot_metrics_curve(df= log_df, metrics = ['MSE'])\n",
    "    plot_metrics_curve(df= log_df, metrics = ['Accuracy(%)'])\n",
    "    plot_metrics_curve(df= log_df, metrics = ['F1(%)'])\n",
    "    plot_metrics_curve(df= log_df, metrics = ['Precision(%)'])\n",
    "    plot_metrics_curve(df= log_df, metrics = ['Recall(%)'])\n",
    "except:\n",
    "    print('没数据')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01628993",
   "metadata": {},
   "source": [
    "## Feature、Logits、Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e525a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:25.426373Z",
     "start_time": "2023-10-26T13:53:18.022913Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_features_AW_and_P_of_one_batch(models, bXs ):\n",
    "    '''直接把所有样本都送进FEM来提特征时，数据量太大会内存溢出,\n",
    "    由于 AW 太占内存了，所以只存储 1 batch 的AW，并返回\n",
    "    '''\n",
    "    ''' 输入一个batch的数据，返回一个batch的features，AW，P\n",
    "    Xs.shape=【bs，pos, depth】，'''\n",
    "    assert len(bXs) <= 100  # 输入数据太多时，可能会内存不够\n",
    "#     print(type(bXs), bXs.shape)\n",
    "    FEM, HCM = models\n",
    "        \n",
    "    if FEM.name in ['ST', 'NET', 'AET']:\n",
    "        bFs, bAWs = FEM( bXs,   training = False, need_w=True)\n",
    "        if  FEM.name=='AET':\n",
    "            bXRC,bFs = bFs\n",
    "    else:\n",
    "        bFs  = FEM( bXs,   training = False)                   # 提取的 特征          (bs, dim)\n",
    "        bAWs = {'No Data': np.zeros((1))} # 用于占位\n",
    "    \n",
    "    bPs  = HCM( bFs,    training = False)                   # 预测 健康状态        (bs, 256，256，4)  feat6，feat7，out8\n",
    "    #print(bFs.shape, bPs.shape, BAWs.keys())\n",
    "    return bFs, bPs, bAWs\n",
    "\n",
    "def get_whole_FP(models, Xs, bs=100, need_save=True):\n",
    "    batch_num = int(np.ceil(len(Xs)/bs) )\n",
    "    print('输入了{:4d}样本,\\t 测试时每{:4d}个样本一个batch @ '.format(len(Xs), bs), batch_num)\n",
    "    all_Fs = []\n",
    "    all_Ps = []\n",
    "    for i in range(batch_num):\n",
    "        bXs = Xs[  i*bs   :   (i+1)*bs   ]\n",
    "        bFs, bPs, bAWs =  get_features_AW_and_P_of_one_batch(models=models, bXs =bXs,   )\n",
    "        all_Fs.append( bFs )\n",
    "        all_Ps.append( bPs )\n",
    "    print('Last batch:',bFs.shape, bPs.shape)\n",
    "    all_Fs = tf.concat( all_Fs, axis=0 ).numpy()\n",
    "    all_Ps = tf.concat( all_Ps, axis=0 ).numpy()\n",
    "    all_P_after_arg_max  = tf.argmax(all_Ps, axis=-1).numpy()\n",
    "    \n",
    "    ### 保存成CSV\n",
    "    if need_save:\n",
    "        Fs_df_columns = ['f_{}'.format(i) for i in range(all_Fs.shape[-1])]\n",
    "        Fs_df =  pd.DataFrame(  data = all_Fs, columns=Fs_df_columns   )\n",
    "        path  =  save_path + get_date() + dta_R_date +'Features.csv'\n",
    "        Fs_df.to_csv(path,  index=False)\n",
    "\n",
    "        Ps_df_columns = ['p_{}'.format(i) for i in range(all_Ps.shape[-1])]\n",
    "        Fs_df =  pd.DataFrame(  data = all_Ps, columns=Ps_df_columns   )\n",
    "        path  =  save_path + get_date() + dta_R_date +'Logits.csv'\n",
    "        Fs_df.to_csv(path,  index=False)\n",
    "    \n",
    "    \n",
    "    return all_Fs, all_Ps, all_P_after_arg_max\n",
    "\n",
    "(traXs,tesXs,  traYs,tesYs), (tra_ds, tes_ds), CLS_NUM = get_batch_from_path(   set_flag = 'CWRU_0HP',  \n",
    "                                                                                file_type='csv', \n",
    "                                                                                strides=5,\n",
    "                                                                                train_rate=0.2,\n",
    "                                                                                seed=19960103,)\n",
    "\n",
    "bX, bY = next(iter(tra_ds))\n",
    "temp_models = get_model(  FEM_name = 'AET',  CLS_NUM  = CLS_NUM,  )\n",
    "\n",
    "\n",
    "bFs, bPs, bAWs =  get_features_AW_and_P_of_one_batch(models=temp_models, bXs = bX,  )\n",
    "print(bFs.shape, bPs.shape, bAWs.keys())\n",
    "\n",
    "\n",
    "traFs,  traPs,  traPargM  =  get_whole_FP( models=temp_models, Xs = traXs, need_save=False  )\n",
    "tesFs,  tesPs,  tesPargM  =  get_whole_FP( models=temp_models, Xs = tesXs, need_save=True  ) # 仅保存测试集上的结果\n",
    "\n",
    "print('traFs.shape, traPs.shape, traPargM.shape', traFs.shape, traPs.shape, traPargM.shape  )\n",
    "print('tesFs.shape, tesPs.shape, traPargM.shape', tesFs.shape, tesPs.shape, traPargM.shape  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e47a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T01:01:20.429262Z",
     "start_time": "2022-09-16T01:01:20.403332Z"
    }
   },
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e93a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:27.185169Z",
     "start_time": "2023-10-26T13:53:25.522404Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_num, item=None,  need_save=True):\n",
    "    print('plot_confusion_matrix 入口： \\t', y_true.shape, y_pred.shape )\n",
    "    assert np.max(y_true)<class_num\n",
    "    assert np.max(y_pred)<class_num\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred) \n",
    "#     print(cm)\n",
    "    \n",
    "    tick_names = Health_status\n",
    "    tick_names = tick_names[:class_num]\n",
    "    plt.figure(figsize=(3.,2.6) )\n",
    "    sns.heatmap(\n",
    "                    data = cm,\n",
    "                    fmt  = '.0f',\n",
    "\n",
    "                    annot=True,\n",
    "                    annot_kws={'size': 9}, \n",
    "                    cmap='Blues',    # Greys  Blues Greens Reds\n",
    "\n",
    "                    linewidths=0.1, \n",
    "                    linecolor='gray',\n",
    "                )\n",
    "\n",
    "    xlocations = np.array(range(class_num))+0.5\n",
    "    plt.ylabel('True label' )\n",
    "    plt.xlabel('Predicted label' )\n",
    "    \n",
    "    plt.xticks(ticks=np.arange(class_num)+0.5, labels=tick_names, rotation=0,)\n",
    "    plt.yticks(ticks=np.arange(class_num)+0.5, labels=tick_names, rotation=0,)\n",
    "\n",
    "    plt.gca().xaxis.set_ticks_position('none') # 屏蔽掉坐标轴的刻度线\n",
    "    plt.gca().yaxis.set_ticks_position('none')\n",
    "    if need_save:\n",
    "        path = save_path + get_date() + dta_R_date + item +'Confusion_Matrix'\n",
    "#         plt.savefig(path  + '.svg', bbox_inches ='tight')\n",
    "        plt.savefig(path  + '.png', bbox_inches ='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "#print('-'*100, target_Y.shape, target_hc_label_P.shape, target_Y, target_hc_label_P)\n",
    "plot_confusion_matrix(      y_true=traYs, \n",
    "                            y_pred=traPargM, \n",
    "                            class_num=CLS_NUM, \n",
    "                            item  =  'tra',  \n",
    "                            need_save=True)\n",
    "\n",
    "plot_confusion_matrix(      y_true=tesYs, \n",
    "                            y_pred=tesPargM, \n",
    "                            class_num=CLS_NUM, \n",
    "                            item  =  'tes',  \n",
    "                            need_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1521b",
   "metadata": {},
   "source": [
    "## Features distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923758a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:29.640867Z",
     "start_time": "2023-10-26T13:53:28.539324Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "'''特征分布图'''\n",
    "def plot_features_distribution(Fs, Ys,  item, class_num, need_save=True):\n",
    "    X_tsne = TSNE(n_components=2,   \n",
    "                  init = 'random',   # 'random', 'pca'  'warn'\n",
    "                  random_state=19960103, \n",
    "                  learning_rate=100    ).fit_transform(Fs)\n",
    "#     print(type(X_tsne), X_tsne.shape) #numpy.ndarray, [sample_number, n_components]\n",
    "\n",
    "    plt.figure(figsize=(1.5,1.5))\n",
    "    a, b = 0.9, 0.5\n",
    "    c, d = '.', '+'\n",
    "\n",
    "    color_list = ['#5B5B5B',     \n",
    "                  'PINK',      'LIME',       'GOLD',     \n",
    "                  'PURPLE',    'AQUA',       'NAVY',     \n",
    "                  'TOMATO',    'SEAGREEN',   'BROWN']\n",
    "    names = ['NC',\n",
    "             'OF1','IF1','RF1',   \n",
    "             'OF2','IF2','RF2',  \n",
    "             'OF3','IF3','RF3']\n",
    "    \n",
    "    for i in range(class_num):\n",
    "        current_components = X_tsne[Ys==i]\n",
    "#         print(i, len(current_components) )\n",
    "        cx = current_components[:, 0]\n",
    "        cy = current_components[:, 1]\n",
    "        # print(cx,cy)\n",
    "        \n",
    "        plt.scatter(    x      = cx,  \n",
    "                        y      = cy, \n",
    "                        marker = c,                    \n",
    "                        color  = color_list[i],    \n",
    "                        label  = names[i],              \n",
    "                        alpha  = a )\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "#     plt.xlabel('Componet 1')\n",
    "#     plt.ylabel('Componet 2')\n",
    "#     plt.legend(ncol=5,\n",
    "#                loc='lower center',               # legend的坐标原点在右下角：4\n",
    "#                bbox_to_anchor=(0.50, 1.02),      # legend的坐标原点与坐标原点的相对坐标\n",
    "#                borderaxespad=0, \n",
    "#                handlelength =1,\n",
    "# #                handletextpad=0.5\n",
    "#               )\n",
    "    plt.tight_layout()\n",
    "    if need_save:\n",
    "        path = save_path + get_date() + dta_R_date + item +'Feature_Distribution'\n",
    "#         plt.savefig(path + '.svg',  bbox_inches ='tight')\n",
    "        plt.savefig(path + '.png',  bbox_inches ='tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_features_distribution(Fs=traFs[::10], Ys=traYs[::10],  item='_tra_', class_num=CLS_NUM, need_save=True)\n",
    "plot_features_distribution(Fs=tesFs[::10], Ys=tesYs[::10],  item='_tes_', class_num=CLS_NUM, need_save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a7a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:30.373648Z",
     "start_time": "2023-10-26T13:53:30.364128Z"
    }
   },
   "outputs": [],
   "source": [
    "tesXs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b3fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:37.173344Z",
     "start_time": "2023-10-26T13:53:30.996557Z"
    }
   },
   "outputs": [],
   "source": [
    "(traXs,tesXs,  traYs,tesYs), (tra_ds, tes_ds), CLS_NUM = get_batch_from_path(   set_flag = 'CWRU_0HP',  \n",
    "                                                                                file_type='csv', \n",
    "                                                                                strides=5,\n",
    "                                                                                train_rate=0.2,\n",
    "                                                                                seed=77,)\n",
    "\n",
    "print(traXs.shape,tesXs.shape)\n",
    "plot_features_distribution(Fs=traXs[:,:,0], Ys=traYs,  item='_tra_rawsignal_', class_num=CLS_NUM, need_save=True)\n",
    "plot_features_distribution(Fs=tesXs[:,:,0], Ys=tesYs,  item='_tes_rawsignal_', class_num=CLS_NUM, need_save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4640fb",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c48544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:53:38.719540Z",
     "start_time": "2023-10-26T13:53:38.707021Z"
    }
   },
   "outputs": [],
   "source": [
    "[i/100 for i in [5,10,15,20,25]   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd8f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:58:33.613773Z",
     "start_time": "2023-10-26T13:58:20.598088Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WC_FLAG1_list = [0,]\n",
    "# WC_FLAG2_list = [0,1,2,3]\n",
    "\n",
    "STRIDES = 5\n",
    "\n",
    "\n",
    "for tes_r in range(1):\n",
    "    for TRAIN_RATE in [i/100 for i in [80]   ]:\n",
    "        for FEM_NAME in  [ 'AET']: # 'MLP', 'BiLSTM',   'CNN','ResNet',   'ST','AET'\n",
    "            for SET_FLAG in [ 'B2022'   ]:               # 'B2022', 'CWRU'  \n",
    "                for WC_FLAG in WC_FLAG1_list:                #  [0,  1,  2,  3]\n",
    "                    '''准备模型和数据的参数'''\n",
    "                    real_set = BJUT_SET_NAME[WC_FLAG] if SET_FLAG=='B2022' else CWRU_SET_NAME[WC_FLAG]\n",
    "                    current_train_model_info = '''\n",
    "                    FEM  = {},\\t  SET_FLAG={},\\t WC_FLAG={},\\t real_set={}, \n",
    "                    TesR = {},\\t  TR = {},\\t '''.format(\n",
    "                        FEM_NAME, SET_FLAG,WC_FLAG,real_set, tes_r, TRAIN_RATE)\n",
    "                    print(current_train_model_info)\n",
    "\n",
    "                    '''准备数据'''\n",
    "                    (traXs,tesXs,  traYs,tesYs), (tra_ds, tes_ds), CLS_NUM = get_batch_from_path( \n",
    "                            set_flag = real_set,  \n",
    "                            strides=5,\n",
    "                            train_rate = TRAIN_RATE,\n",
    "                            seed = tes_r, )\n",
    "\n",
    "\n",
    "                    '''每次完整的训练，都重置一下Model和OPT的参数'''\n",
    "                    models = get_model( FEM_name=FEM_NAME,   CLS_NUM=CLS_NUM )\n",
    "\n",
    "                    LEARNING_RATE = 0.001\n",
    "                    optimizer  = tf.keras.optimizers.Adam(LEARNING_RATE )\n",
    "\n",
    "                    '''训练，并返回训练好的 模型 and 训练日志 '''\n",
    "                    models, tra_tes_metric_logs = train_model(tra_ds=tra_ds,\n",
    "                                                              tes_ds=tes_ds,\n",
    "                                                              models=models,\n",
    "                                                              epoch_num=10,\n",
    "                                                              steps_per_epoch=2, \n",
    "                                                              val_steps_per_epoch=1,\n",
    "                                                              info = current_train_model_info,\n",
    "                                                             )\n",
    "                    \n",
    "                    '''存储模型权重和训练日志'''                \n",
    "                    dta_R_date = '______[' + FEM_NAME +  ']__[TRA_SET='+real_set+ ']__[TES_R'+str(tes_r) + ']__[TRAIN_RATE='+str(TRAIN_RATE) +']_____'\n",
    "                    log_df,log_path = save_weights_and_log()\n",
    "\n",
    "                    '''绘制训练曲线'''\n",
    "                    log_df = pd.read_csv(  log_path  )\n",
    "                    plot_metrics_curve(df=log_df, metrics=['CE'])\n",
    "                    plot_metrics_curve(df=log_df, metrics=['MSE'])\n",
    "                    plot_metrics_curve(df=log_df, metrics=['Accuracy(%)'])\n",
    "                    plot_metrics_curve(df=log_df, metrics=['F1(%)'])\n",
    "                    plot_metrics_curve(df=log_df, metrics=['Precision(%)'])\n",
    "                    plot_metrics_curve(df=log_df, metrics=['Recall(%)'])\n",
    "\n",
    "                    '''在训练数据集 和 测试数据集 上 测试模型最终的性能'''\n",
    "                    traFs,  traPs,  traPargM = get_whole_FP(models=models, Xs=traXs, )\n",
    "                    tesFs,  tesPs,  tesPargM = get_whole_FP(models=models, Xs=tesXs, )\n",
    "                    print('traFs.shape, traPs.shape, traPargM.shape', traFs.shape, traPs.shape, traPargM.shape)\n",
    "                    print('tesFs.shape, tesPs.shape, traPargM.shape', tesFs.shape, tesPs.shape, traPargM.shape)\n",
    "\n",
    "                    item = '____[TraSET={0}-{1}]-[TesSET={2}-{3}]____'.format(SET_FLAG,WC_FLAG, SET_FLAG,WC_FLAG)\n",
    "                    save_test_result(item = item)\n",
    "\n",
    "\n",
    "                    '''绘制混淆矩阵'''\n",
    "                    plot_confusion_matrix(y_true=traYs,\n",
    "                                          y_pred=traPargM,\n",
    "                                          class_num=CLS_NUM,\n",
    "                                          item=item+'_TRA___',\n",
    "                                          need_save=True)\n",
    "\n",
    "                    plot_confusion_matrix(y_true=tesYs,\n",
    "                                          y_pred=tesPargM,\n",
    "                                          class_num=CLS_NUM,\n",
    "                                          item=item+'_TES___',\n",
    "                                          need_save=True)\n",
    "\n",
    "                    '''绘制特征分布图'''\n",
    "                    plot_features_distribution(  Fs=traFs, Ys=traYs,  item=item+'_TRA___', class_num=CLS_NUM, need_save=True)\n",
    "                    plot_features_distribution(  Fs=tesFs, Ys=tesYs,  item=item+'_TES___', class_num=CLS_NUM, need_save=True)\n",
    "\n",
    "                    print(current_train_model_info)\n",
    "                    print('\\nOne Epoch Done!!!\\n', '#'*500, '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a4072",
   "metadata": {},
   "source": [
    "# 结果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fca8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:58:36.824669Z",
     "start_time": "2023-10-26T13:58:36.815149Z"
    }
   },
   "outputs": [],
   "source": [
    "traXs.shape, traYs.shape, tesXs.shape, tesYs.shape, traYs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3a1ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:58:38.706878Z",
     "start_time": "2023-10-26T13:58:38.682816Z"
    }
   },
   "outputs": [],
   "source": [
    "each_class_X = [traXs[traYs==i] for i in range(10)]\n",
    "each_class_Y = [traYs[traYs==i] for i in range(10)]\n",
    "each_class_N = [len(traYs[traYs==i]) for i in range(10)]\n",
    "\n",
    "X = tf.concat(each_class_X, axis=0).numpy()\n",
    "Y = tf.concat(each_class_Y, axis=0).numpy()\n",
    "X.shape, Y[::80], each_class_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eee26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:59:01.226349Z",
     "start_time": "2023-10-26T13:58:59.465642Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_indexes = [72, 1*80+6, 2*80+56, 80*3+5, 80*4, 80*5+11, 80*6 ]\n",
    "target_signals = X[target_indexes]\n",
    "target_lables  = Y[target_indexes]\n",
    "\n",
    "x_sine  = np.sin(np.linspace(0,150,2048)).reshape(-1,1)\n",
    "\n",
    "################################################################\n",
    "x_on    = np.stack([target_signals[1]]*10)\n",
    "s = 650\n",
    "k = 800\n",
    "x_on[:5,s:s+k]    = np.random.normal(loc=0.0, scale=1.0, size=(5,k,1))\n",
    "x_on[5:,]         += np.random.normal(loc=0.0, scale=1.0, size=(5,2048,1))\n",
    "\n",
    "x_in    = np.stack([target_signals[2]]*10)\n",
    "s = 650\n",
    "k = 800\n",
    "x_in[:5,s:s+k]    = np.random.normal(loc=0.0, scale=1.0, size=(5,k,1))\n",
    "x_in[5:,]         += np.random.normal(loc=0.0, scale=1.0, size=(5,2048,1))\n",
    "################################################################\n",
    "\n",
    "simu_indexes = np.array(  [-1*i for i in range(1,21)])\n",
    "simu_signals = np.concatenate([x_on, x_in])\n",
    "simu_lables  = np.array( [-1,]*10+[-2,]*10)\n",
    "\n",
    "target_indexes = np.concatenate([target_indexes, simu_indexes])\n",
    "target_signals = np.concatenate([target_signals, simu_signals])\n",
    "target_lables  = np.concatenate([target_lables,  simu_lables])\n",
    "\n",
    "print( x_sine.shape, x_in.shape, target_signals.shape)\n",
    "\n",
    "for i in range(len(target_indexes)):\n",
    "    idx = target_indexes[i]\n",
    "    x = target_signals[i]\n",
    "    y = target_lables[i]\n",
    "    \n",
    "    plt.figure(figsize=(5,1))\n",
    "    plt.plot(x,linewidth=0.5)\n",
    "    plt.title('idx={},lb={}'.format(idx, y))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4de6ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:59:04.045349Z",
     "start_time": "2023-10-26T13:59:03.959708Z"
    }
   },
   "outputs": [],
   "source": [
    "each_class_X = [traXs[traYs==i] for i in range(10)]\n",
    "each_class_Y = [traYs[traYs==i] for i in range(10)]\n",
    "each_class_N = [len(traYs[traYs==i]) for i in range(10)]\n",
    "\n",
    "X = tf.concat(each_class_X, axis=0).numpy()\n",
    "Y = tf.concat(each_class_Y, axis=0).numpy()\n",
    "print(X.shape, Y[::80], each_class_N)\n",
    "\n",
    "s = 650\n",
    "k = 800\n",
    "\n",
    "X_1, Y_1 = np.array(X),np.array(Y)\n",
    "X_2, Y_2 = np.array(X),np.array(Y)\n",
    "\n",
    "X_1[:,s:s+k]     = np.random.normal(loc=0.0, scale=1.0, size=(len(X),k,1))\n",
    "X_2             += np.random.normal(loc=0.0, scale=1.0, size=(len(X),2048,1))\n",
    "\n",
    "X = np.concatenate([X, X_1, X_2])\n",
    "Y = np.concatenate([Y, Y_1, Y_2])\n",
    "X.shape, Y[::80], Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73100e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:59:53.153935Z",
     "start_time": "2023-10-26T13:59:52.855157Z"
    }
   },
   "outputs": [],
   "source": [
    "FEM, HCM = models\n",
    "(xrc, feature), aw = FEM(each_class_X[0], training=False, need_w=True)\n",
    "print([aw.shape for aw in list(aw.values())])\n",
    "aww = tf.stack(list(aw.values()), axis=1)\n",
    "xrc.shape, feature.shape, aw.keys(),aww.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f34de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T13:59:56.354818Z",
     "start_time": "2023-10-26T13:59:54.759073Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_num = int(np.ceil(len(X)/100))\n",
    "each_batch_T  = [FEM(X[i*100:(i+1)*100], training=False, need_w=True) for i in range(batch_num) ]\n",
    "each_batch_XR = [i[0][0] for i in each_batch_T]\n",
    "each_batch_F  = [i[0][1] for i in each_batch_T]\n",
    "each_batch_AW = [i[1] for i in each_batch_T]\n",
    "each_batch_AW = [tf.stack(list(i.values()), axis=1) for i in each_batch_AW]\n",
    "each_batch_P  = [tf.argmax(HCM(i, training=False), axis=1)    for i in each_batch_F]\n",
    "each_batch_LG  = [HCM(i, training=False)    for i in each_batch_F]\n",
    "\n",
    "XR = tf.concat(each_batch_XR, axis=0).numpy()\n",
    "F  = tf.concat(each_batch_F,  axis=0).numpy()\n",
    "AW = tf.concat(each_batch_AW, axis=0).numpy()\n",
    "P  = tf.concat(each_batch_P,  axis=0).numpy()\n",
    "LG = tf.concat(each_batch_LG,  axis=0).numpy()\n",
    "\n",
    "XR.shape, F.shape, AW.shape, P.shape, LG.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2059b1",
   "metadata": {},
   "source": [
    "## 注意力可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973dd03",
   "metadata": {},
   "source": [
    "## 不保存CSV运行的嘎嘎快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92235d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:00:10.358113Z",
     "start_time": "2023-10-26T14:00:08.657498Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp = './save/2023_8_14_att/temp/11/'\n",
    "num2name = {0:'NC', 1:'OF', 2:'IF', 3:'RF'}\n",
    "def scale(x):\n",
    "    ma = np.max(x)\n",
    "    mi = np.min(x)\n",
    "    return (x-mi)/(ma-mi)\n",
    "\n",
    "def plotone_sample_and_its_attention(idx,  need_show = True, need_save = False ):\n",
    "    signal = X[idx].reshape(-1)\n",
    "    logit  = LG[idx].reshape(-1)\n",
    "    lb     = Y[idx]\n",
    "    \n",
    "    xrc    = XR[idx].reshape(-1)\n",
    "    att    = AW[idx]\n",
    "    layer_num,header_num,_,_ = att.shape\n",
    "    domain = 'B0'\n",
    "    \n",
    "    \n",
    "    '''Signal'''\n",
    "    if need_show:\n",
    "        plt.figure(figsize=(5,1),dpi=100)\n",
    "        plt.plot(signal, linewidth=0.8)\n",
    "        plt.xlim([0, len(signal)])\n",
    "        \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        if need_save:\n",
    "            name = '0-{} - C_{} - I_{} -noise_{}- Signal.png'.format(domain, lb, idx, idx//(7*80))\n",
    "            plt.savefig(sp + name, bbox_inches ='tight', dpi=600)\n",
    "            \n",
    "    '''Reconstructed Signal'''\n",
    "    if need_show:\n",
    "        plt.figure(figsize=(5,1),dpi=100)\n",
    "        plt.plot(xrc, linewidth=0.8, color='g')\n",
    "        plt.xlim([0, len(xrc)])\n",
    "        \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        if need_save:\n",
    "            name = '1-{} - C_{} - I_{} - Signal_rc.png'.format(domain, lb, idx)\n",
    "            plt.savefig(sp + name, bbox_inches ='tight', dpi=600)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Attention map'''\n",
    "    if need_show:\n",
    "        plt.figure(figsize=(header_num,layer_num),dpi=100)\n",
    "        for layer_idx in range(layer_num):\n",
    "            for header_idx in range(header_num):\n",
    "                a = att[layer_idx,header_idx]\n",
    "                ii = layer_idx*4 + header_idx+1\n",
    "\n",
    "                plt.subplot(layer_num,header_num,ii)\n",
    "                plt.contourf( a, levels=16, cmap='inferno')\n",
    "                plt.axis('off')\n",
    "        if need_save:\n",
    "            name = '2-{} - C_{} - I_{} - All_att.png'.format(domain, lb, idx)\n",
    "            plt.savefig(sp + name, bbox_inches ='tight', dpi=600)\n",
    "        plt.show()\n",
    "        \n",
    "    '''layer mean Attention map '''\n",
    "    if need_show:\n",
    "        plt.figure(figsize=(layer_num,1),dpi=100)\n",
    "        for layer_idx in range(layer_num):\n",
    "            layer_a = att[layer_idx]\n",
    "            mean_layer_a = np.mean(layer_a, axis=0)\n",
    "            ii = layer_idx+1\n",
    "\n",
    "            plt.subplot(1,layer_num,ii)\n",
    "            plt.contourf( mean_layer_a, levels=16, cmap='inferno')\n",
    "            plt.axis('off')\n",
    "        if need_save:\n",
    "            name = '7-{} - C_{} - I_{} - Mean_layer_att.png'.format(domain, lb, idx)\n",
    "            plt.savefig(sp + name, bbox_inches ='tight', dpi=600)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    '''Last layer attention map'''\n",
    "    llam = np.mean(att[-1,:,:],axis=0)\n",
    "    if need_show:\n",
    "        plt.figure(figsize=(2,2),dpi=100)\n",
    "        contour_plot = plt.contourf( llam, levels=16, cmap='inferno',cbar=True)\n",
    "\n",
    "        # Add a colorbar\n",
    "        colorbar = plt.colorbar(contour_plot)\n",
    "\n",
    "        plt.xlabel('Key' )\n",
    "        plt.ylabel('Query' )\n",
    "        if need_save:\n",
    "            name = '3-{} - C_{} - I_{} - LLAM.png'.format(domain, lb, idx)\n",
    "            plt.savefig(sp + name, bbox_inches ='tight', dpi=600)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "#     '''QKA'''\n",
    "    a2q = np.mean(llam,axis=1, keepdims=True)  # (T,1)\n",
    "    a2k = np.mean(llam,axis=0, keepdims=True)  # (1,T)\n",
    "# #     print(a2q.shape, a2k.shape)\n",
    "\n",
    "    if need_show:\n",
    "        plt.figure(figsize=(5,1),dpi=100)\n",
    "        plt.plot(a2q.reshape(-1)*100, label='Query')\n",
    "        plt.plot(a2k.reshape(-1)*100, label='Key')\n",
    "        mi = np.min([np.min(a2q),np.min(a2k)])*100\n",
    "        ma = np.max([np.max(a2q),np.max(a2k)])*100\n",
    "        \n",
    "        tile = 0.15*(ma-mi)\n",
    "        mi2 = mi-tile\n",
    "        ma2 = ma+tile\n",
    "        text_y = ma + 2*tile\n",
    "        \n",
    "        plt.text(x=0,y=text_y,s='$×10^{-2}$',fontdict={'family': 'Times New Roman', 'size': 8})\n",
    "        plt.xlim([0, len(a2q)])\n",
    "        plt.ylim(mi2, ma2)\n",
    "        plt.xlabel('Position')\n",
    "        plt.ylabel('Attention')\n",
    "        plt.legend(ncol=2,\n",
    "                   loc='lower center',        \n",
    "                   bbox_to_anchor=(0.70, 1.0))            \n",
    "        if need_save:\n",
    "            name = '4-{} - C_{} - I_{} - QK.png'.format(domain, lb, idx)\n",
    "            plt.savefig(sp + name, bbox_inches ='tight', dpi=600)\n",
    "        plt.show()\n",
    "            \n",
    "\n",
    "    '''Signal attention'''\n",
    "    qa = np.array([[i]*32 for i in a2q[1:, 0 ]]) # [64,32]  跳过 class token\n",
    "    ka = np.array([[i]*32 for i in a2k[0,  1:]]) # [64,32]\n",
    "#     print(qa.shape,ka.shape)\n",
    "\n",
    "    qa = scale(qa.reshape(-1))\n",
    "    ka = scale(ka.reshape(-1))\n",
    "    signal = scale(signal)*2-1\n",
    "\n",
    "    plt.figure(figsize=(5,1),dpi=100)\n",
    "    plt.plot(signal,          label='Signal',    linewidth=0.8)\n",
    "    plt.plot(ka ,             label='Attention', linewidth=1.5, alpha=0.75)\n",
    "    plt.xlim([0, len(signal)])\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "#     plt.ylabel(lb)\n",
    "    plt.xticks([])\n",
    "#     plt.legend(ncol=2,\n",
    "#                loc='lower center',        \n",
    "#                bbox_to_anchor=(0.50, 1.0))    \n",
    "#     plt.grid(axis='y')\n",
    "#     plt.ylabel('Scaled value')\n",
    "    if need_save:\n",
    "        name = '5-{} - C_{} - I_{} - Signal_and_Attention.png'.format(domain, lb, idx)\n",
    "        plt.savefig(sp + name, bbox_inches ='tight', dpi=600)\n",
    "    plt.show()\n",
    "    \n",
    "#     '''把信号，重构信号，注意力矩阵存储起来'''    \n",
    "#     if need_save:\n",
    "#         name = '6-{} - C_{} - I_{} - x_xrc_att.xlsx'.format(domain, lb, idx)\n",
    "#         value = np.concatenate([signal,xrc,att.reshape(-1)],axis=0)\n",
    "#         key = 'x_{}__xrc_{}__att_{}'.format(len(signal),len(xrc),att.shape)\n",
    "#         data_dict = {\n",
    "#             'x_{}'.format(len(signal)):signal,\n",
    "#             'logit_{}'.format(len(signal)):logit,\n",
    "#             'y':[lb],\n",
    "#             'xrc_{}'.format(len(xrc)):xrc,\n",
    "#             'att_{}'.format(att.shape):att.reshape(-1),\n",
    "#             'llam_{}'.format(llam.shape):llam.reshape(-1),\n",
    "#             'qa_{}'.format(len(qa)):qa,\n",
    "#             'ka_{}'.format(len(ka)):ka,\n",
    "#         } \n",
    "        \n",
    "#         df = pd.DataFrame.from_dict(data_dict, orient='index').T\n",
    "#         df.to_excel(sp + name, float_format = '%.6f', index=False)\n",
    "    \n",
    "#     print('@'*100)\n",
    "    return\n",
    "\n",
    "\n",
    "jk = 40\n",
    "for i in [jk]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = True, need_save = 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e370f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:00:22.975922Z",
     "start_time": "2023-10-26T14:00:17.542377Z"
    }
   },
   "outputs": [],
   "source": [
    "# 外圈 0.5mm\n",
    "jk = 128\n",
    "for i in [jk, jk+7*80*1, jk+7*80*2]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = True, need_save = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33970edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:00:53.894098Z",
     "start_time": "2023-10-26T14:00:49.237128Z"
    }
   },
   "outputs": [],
   "source": [
    "# 内圈 0.5mm\n",
    "jk = 200\n",
    "for i in [jk, jk+7*80*1, jk+7*80*2]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = True, need_save = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e686d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:01:00.538156Z",
     "start_time": "2023-10-26T14:00:55.303867Z"
    }
   },
   "outputs": [],
   "source": [
    "# 滚子 0.5mm\n",
    "jk = 304\n",
    "for i in [jk, jk+7*80*1, jk+7*80*2]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = True, need_save = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa11dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:01:07.528094Z",
     "start_time": "2023-10-26T14:01:01.973909Z"
    }
   },
   "outputs": [],
   "source": [
    "# 外圈 2mm\n",
    "jk = 368\n",
    "for i in [jk, jk+7*80*1, jk+7*80*2]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = True, need_save = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab06f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:01:14.297943Z",
     "start_time": "2023-10-26T14:01:08.894371Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 内圈 2mm\n",
    "jk = 472\n",
    "for i in [jk, jk+7*80*1, jk+7*80*2]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = True, need_save = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24a444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:01:20.729097Z",
     "start_time": "2023-10-26T14:01:15.776354Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 滚子 2mm\n",
    "jk = 496\n",
    "for i in [jk, jk+7*80*1, jk+7*80*2]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = True, need_save = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf2a86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:01:22.479117Z",
     "start_time": "2023-10-26T14:01:22.121746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [ 0*80+72, 1*80+6, 2*80+56, 80*3+5, 80*4, 80*5+11, 80*6  ]:\n",
    "    plotone_sample_and_its_attention(idx=i,  need_show = 0, need_save = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914e197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "348px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
